{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environnement\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glove-global-vectors-for-word-representation', 'sentence-relatedness']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field,ReversibleField,TabularDataset,Iterator,BucketIterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "Lecture des données\n",
    "==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112 A man is opening a package that contains headphones || There is no man singing and playing the guitar 0.24\n",
      "8601 Little boys are playing in a water fountain in front of lots of people || Children are playing in a fountain that is spraying water from the ground 0.74\n",
      "588 The children are playing in front of a large door || A group of boys are quiet in front of a large door made of wood 0.6799999999999999\n",
      "5012 A man is cutting an onion || An onion is being sliced by a woman 0.76\n",
      "9051 A pair of kids are sticking out blue and green colored tongues || Two kids are sticking out blue and green colored tongues 0.96\n",
      "\n",
      "6 There is no boy playing outdoors and there is no man smiling || A group of kids is playing in a yard and an old man is standing in the background\n",
      "7 A group of boys in a yard is playing and a man is standing in the background || The young boys are playing outdoors and the man is smiling nearby\n",
      "8 A group of children is playing in the house and there is no man standing in the background || The young boys are playing outdoors and the man is smiling nearby\n",
      "10 A brown dog is attacking another animal in front of the tall man in pants || A brown dog is attacking another animal in front of the man in pants\n",
      "11 A brown dog is attacking another animal in front of the man in pants || A brown dog is helping another animal in front of the man in pants\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def proc_float(value):\n",
    "    return float(value)\n",
    "\n",
    "def proc_int(value):\n",
    "    return int(value)\n",
    "\n",
    "# lowercase the corpus\n",
    "TEXT      = Field(sequential=True, lower=False, tokenize=tokenize) #might alternatively specify cuda data types to get the dataset to live permanently on the GPU\n",
    "FLOAT     = Field(sequential=False, use_vocab=False,dtype=torch.float,preprocessing=proc_float) \n",
    "INTEGER   = Field(sequential=False, use_vocab=False,preprocessing=proc_int)\n",
    "\n",
    "df         = TabularDataset(\"../input/sentence-relatedness/SICK_train_logistic.txt\",\"tsv\",skip_header=True,\\\n",
    "                            fields=[('idx',INTEGER),('sentA',TEXT),('sentB',TEXT),('Relatedness',FLOAT)])\n",
    "\n",
    "df_train,df_dev  = df.split(split_ratio=0.8)\n",
    "TEXT.build_vocab(df_train)\n",
    "\n",
    "#Prints out the first few lines of the train set\n",
    "for elt in df_train[:5]:\n",
    "    print(elt.idx,' '.join(elt.sentA),'||',' '.join(elt.sentB),elt.Relatedness)\n",
    "print()\n",
    "\n",
    "#load test set\n",
    "df_test = TabularDataset(\"../input/sentence-relatedness/SICK_test.txt\",\"tsv\",skip_header=True,\\\n",
    "                            fields=[('idx',INTEGER),('sentA',TEXT),('sentB',TEXT)])\n",
    "\n",
    "#Prints out the first few lines of the test set\n",
    "for elt in df_test[:5]:\n",
    "    print(elt.idx,' '.join(elt.sentA),'||',' '.join(elt.sentB))\n",
    "print()\n",
    "print(df_test[0].idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,hidden_dim,embedding_dim):\n",
    "       \n",
    "        super(ParaphraseClassifier, self).__init__()\n",
    "       \n",
    "        self.hidden_dim    = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding     = nn.Embedding(len(TEXT.vocab), embedding_dim)\n",
    "        self.lstm          = nn.LSTM(embedding_dim, hidden_dim, num_layers=1,bidirectional=False)\n",
    "        self.Wadd          = nn.Linear(hidden_dim,hidden_dim)   \n",
    "        self.Wtimes        = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.Wout          = nn.Linear(hidden_dim,1)\n",
    "        \n",
    "        \n",
    "    def use_glove_embeddings(self):\n",
    "        \n",
    "        # load Glove\n",
    "        embeddings_dict = {}\n",
    "        with open(\"../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\", 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                embeddings_dict[word] = vector\n",
    "        \n",
    "        # adapt to corpus\n",
    "        weights_matrix = []\n",
    "        words_found = 0\n",
    "\n",
    "        vocab = [TEXT.vocab.itos[i] for i in range(len(TEXT.vocab))]\n",
    "\n",
    "        for i, word in enumerate(vocab):\n",
    "            try: \n",
    "                weights_matrix.append(embeddings_dict[word])\n",
    "                words_found += 1\n",
    "            # pour les rares mots qui ne sont pas dans Glove, on laisse un vecteur au hasard\n",
    "            except KeyError:\n",
    "                weights_matrix.append(np.random.normal(scale=0.6, size=(50, )))\n",
    "\n",
    "        weights_matrix = torch.FloatTensor(weights_matrix)\n",
    "                \n",
    "        # create layer\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights_matrix)\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self,xinputA,xinputB):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xinputA is a sequence of word indexes\n",
    "            xinputB is a sequence of word indexes\n",
    "        The forward method also works for batched input.       \n",
    "        \"\"\"\n",
    "        ##details for dimensionalities\n",
    "        #embeddings\n",
    "        #  input : batch_size x seq_length\n",
    "        #  output: batch-size x seq_length x embedding_dimension\n",
    "        #lstm\n",
    "        #  input : seq_length x batch_size x embedding_size\n",
    "        #  output: seq_length x batch_size x hidden_size  (for the sequence)\n",
    "        #  output: batch_size x hidden_size (for the last hidden/cell state)\n",
    "        xembeddedA                       = self.embedding(xinputA)                                                #catches embedding vectors\n",
    "        lstm_outA, (hiddenA,cellA)       = self.lstm(xembeddedA.view(len(xinputA), -1, self.embedding_dim), None) #-1 is a wildcard (here we let pytorch guess batch size)\n",
    "       \n",
    "        xembeddedB                       = self.embedding(xinputB)                                                #catches embedding vectors\n",
    "        lstm_outB, (hiddenB,cellB)       = self.lstm(xembeddedB.view(len(xinputB), -1, self.embedding_dim), None)\n",
    "       \n",
    "        #hiddenA = hiddenA.view(-1,self.hidden_dim * 2)\n",
    "        #hiddenB = hiddenB.view(-1,self.hidden_dim * 2)       \n",
    "        #merge sentence representations\n",
    "        hiddenT = hiddenA * hiddenB\n",
    "        hiddenD = torch.abs(hiddenA - hiddenB)\n",
    "        hidden  = torch.tanh(self.Wtimes(hiddenT) + self.Wadd(hiddenD))\n",
    "        return torch.sigmoid(self.Wout(hidden))\n",
    "    \n",
    "    \n",
    "    def train(self,train_set,dev_set,epochs,learning_rate=0.001):\n",
    "        \n",
    "        loss_func  = nn.BCELoss() \n",
    "#         optimizer  = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        optimizer  = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # la vitesse d'entraînement est décuplée en passant de batchs de 1 à des batchs de 64\n",
    "        train_iterator   = BucketIterator(train_set, batch_size=64, device=-1, sort_key=lambda x: len(x.sentA), sort=False, sort_within_batch=False, repeat=False)\n",
    "        \n",
    "        t=time.time()\n",
    "        for e in range(epochs):\n",
    "            global_logloss = 0\n",
    "            nb_batch = 0\n",
    "            for i, batch in enumerate(train_iterator):\n",
    "                nb_batch += 1\n",
    "                xvecA,xvecB,yRelness = batch.sentA,batch.sentB,batch.Relatedness\n",
    "                self.zero_grad()\n",
    "                prob            = self.forward(xvecA,xvecB).squeeze()\n",
    "                loss            = loss_func(prob,yRelness)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                global_logloss += loss.item()\n",
    "            \n",
    "            average_loss = global_logloss/nb_batch\n",
    "            print(\"Epoch %d, mean cross entropy = %f\"%(e+1,average_loss))\n",
    "                \n",
    "        print(epochs,\"epochs,\",time.time()-t,'s')\n",
    "            \n",
    "            \n",
    "    def run_test(self,test_set):\n",
    "\n",
    "        # make predictions\n",
    "        test_iterator   = Iterator(test_set, batch_size=1, device=-1, sort=False, sort_within_batch=False, repeat=False, shuffle=False)\n",
    "        \n",
    "        predictions = list()\n",
    "        idx = list()\n",
    "        for elt in test_iterator:\n",
    "            xvecA,xvecB = elt.sentA,elt.sentB\n",
    "            relness     = self.forward(xvecA,xvecB).squeeze()\n",
    "            score       = relness.item() *4 +1\n",
    "            predictions.append(score)\n",
    "            idx.append(elt.idx.item())\n",
    "\n",
    "        to_df = {'pairID':idx,'Relatedness': predictions}\n",
    "        df = pd.DataFrame(to_df)\n",
    "        print(df)\n",
    "        df.to_csv('results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean cross entropy = 0.616256\n",
      "Epoch 2, mean cross entropy = 0.607720\n",
      "Epoch 3, mean cross entropy = 0.607006\n",
      "Epoch 4, mean cross entropy = 0.605743\n",
      "Epoch 5, mean cross entropy = 0.605694\n",
      "Epoch 6, mean cross entropy = 0.601661\n",
      "Epoch 7, mean cross entropy = 0.583989\n",
      "Epoch 8, mean cross entropy = 0.576904\n",
      "Epoch 9, mean cross entropy = 0.572215\n",
      "Epoch 10, mean cross entropy = 0.568487\n",
      "Epoch 11, mean cross entropy = 0.564155\n",
      "Epoch 12, mean cross entropy = 0.560519\n",
      "Epoch 13, mean cross entropy = 0.557589\n",
      "Epoch 14, mean cross entropy = 0.553696\n",
      "Epoch 15, mean cross entropy = 0.551427\n",
      "Epoch 16, mean cross entropy = 0.549519\n",
      "Epoch 17, mean cross entropy = 0.546295\n",
      "Epoch 18, mean cross entropy = 0.543770\n",
      "Epoch 19, mean cross entropy = 0.541558\n",
      "Epoch 20, mean cross entropy = 0.540912\n",
      "Epoch 21, mean cross entropy = 0.538156\n",
      "Epoch 22, mean cross entropy = 0.536383\n",
      "Epoch 23, mean cross entropy = 0.534205\n",
      "Epoch 24, mean cross entropy = 0.531019\n",
      "Epoch 25, mean cross entropy = 0.530796\n",
      "Epoch 26, mean cross entropy = 0.529317\n",
      "Epoch 27, mean cross entropy = 0.527438\n",
      "Epoch 28, mean cross entropy = 0.525429\n",
      "Epoch 29, mean cross entropy = 0.524552\n",
      "Epoch 30, mean cross entropy = 0.520597\n",
      "Epoch 31, mean cross entropy = 0.520985\n",
      "Epoch 32, mean cross entropy = 0.520344\n",
      "Epoch 33, mean cross entropy = 0.518393\n",
      "Epoch 34, mean cross entropy = 0.517054\n",
      "Epoch 35, mean cross entropy = 0.516226\n",
      "Epoch 36, mean cross entropy = 0.517308\n",
      "Epoch 37, mean cross entropy = 0.514070\n",
      "Epoch 38, mean cross entropy = 0.513669\n",
      "Epoch 39, mean cross entropy = 0.512440\n",
      "Epoch 40, mean cross entropy = 0.512221\n",
      "Epoch 41, mean cross entropy = 0.510346\n",
      "Epoch 42, mean cross entropy = 0.511113\n",
      "Epoch 43, mean cross entropy = 0.509052\n",
      "Epoch 44, mean cross entropy = 0.509515\n",
      "Epoch 45, mean cross entropy = 0.508943\n",
      "Epoch 46, mean cross entropy = 0.508077\n",
      "Epoch 47, mean cross entropy = 0.508216\n",
      "Epoch 48, mean cross entropy = 0.506912\n",
      "Epoch 49, mean cross entropy = 0.506809\n",
      "Epoch 50, mean cross entropy = 0.507350\n",
      "50 epochs, 213.10839653015137 s\n",
      "      pairID  Relatedness\n",
      "0          6     3.542763\n",
      "1          7     4.378931\n",
      "2          8     3.758984\n",
      "3         10     4.524432\n",
      "4         11     4.138576\n",
      "...      ...          ...\n",
      "4922    9991     3.933788\n",
      "4923    9992     2.219187\n",
      "4924    9994     2.719530\n",
      "4925    9995     2.382106\n",
      "4926    9996     2.663532\n",
      "\n",
      "[4927 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "pc = ParaphraseClassifier(150,50)\n",
    "pc.use_glove_embeddings()\n",
    "\n",
    "pc.train(df_train,df_dev,50)\n",
    "\n",
    "pc.run_test(df_test)\n",
    "\n",
    "# meilleur score :\n",
    "# emb_sz=30, epoch=150, lr=0.001\n",
    "# opt=Adam\n",
    "\n",
    "# V8\n",
    "# - batch dans run_test remis à 1\n",
    "# - passage de Relatedness [0,1] à score [1,5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
